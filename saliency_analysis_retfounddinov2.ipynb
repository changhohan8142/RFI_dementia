{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f870e1-61df-4332-9c1f-48016345d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Grad-CAM export — ALL ARCH × FT combos\n",
    "# - Robust attention capture (hooks → fallback override), RoPE-safe for DINOv3\n",
    "# - Handles non-square token grids (pads/trims to nearest square)\n",
    "# - FOV-masked averaging for L/R heatmaps\n",
    "# - Computes mask metrics & ATTN summaries\n",
    "\n",
    "import os, sys, math, json, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- project-local imports (same as trainer.py / gradcam_dementia.py)\n",
    "WORK_DIR   = \"/home/hch/dementia\"   # for importing local modules\n",
    "MASTER_CSV = \"/home/hch/opportunistic/20250518_master4_merged2.csv\"  # for eye_orientation merge\n",
    "\n",
    "# segmentation roots\n",
    "ARTERY_DIR = Path(\"/home/hch/opportunistic/AutoMorph_Data/Results/M2/artery_vein/artery_binary_process\")\n",
    "VEIN_DIR   = Path(\"/home/hch/opportunistic/AutoMorph_Data/Results/M2/artery_vein/vein_binary_process\")\n",
    "BINVES_DIR = Path(\"/home/hch/opportunistic/AutoMorph_Data/Results/M2/binary_vessel/binary_process\")  # optional\n",
    "ODC_RAW_DIR= Path(\"/home/hch/opportunistic/AutoMorph_Data/Results/M2/optic_disc_cup/raw\")\n",
    "\n",
    "# image size & batch\n",
    "IMG_SIZE   = 448\n",
    "BATCH      = 8\n",
    "\n",
    "# make paths consistent with existing codebase\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "from data import DementiaDetectionDataset, DementiaPredictionDataset\n",
    "from models.encoder import build_model\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) Config: all combos\n",
    "# -----------------------------------------------------------------------------\n",
    "ARCHES = ['retfound_dinov2']\n",
    "FTS    = ['partial', 'lora']\n",
    "\n",
    "def ft_blks_for(ft: str):\n",
    "    if ft == 'partial': return 4\n",
    "    if ft == 'lora':    return 'full'\n",
    "    return None\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Utilities: MODEL_DESC, CKPT/INFER paths\n",
    "# -----------------------------------------------------------------------------\n",
    "def model_desc_from_csv_row(row: pd.Series) -> str:\n",
    "    arch = str(row['arch']); ft = str(row['ft'])\n",
    "    desc = f\"{arch}_{ft}\"\n",
    "    if ft == 'partial':\n",
    "        desc += f\"_ft_{row['ft_blks']}\"\n",
    "    elif ft == 'lora':\n",
    "        lrk = str(row.get('lora_rank', ''))\n",
    "        desc += f\"_rank_{lrk}_ft_{row['ft_blks']}\"\n",
    "    return desc\n",
    "\n",
    "def model_desc_from_spec(arch: str, ft: str, lora_rank: str | int | None) -> str:\n",
    "    desc = f\"{arch}_{ft}\"\n",
    "    if ft == 'partial':\n",
    "        desc += f\"_ft_4\"\n",
    "    elif ft == 'lora':\n",
    "        lrk = str(lora_rank if lora_rank is not None else \"4\")\n",
    "        desc += f\"_rank_{lrk}_ft_full\"\n",
    "    return desc\n",
    "\n",
    "def ckpt_path(desc: str) -> str:\n",
    "    return f\"/home/kjw/Projects/dementia/ckpts/{desc}/ckpt.pth.tar\"\n",
    "\n",
    "def infer_dir(desc: str) -> str:\n",
    "    return f\"/home/hch/dementia/infer_out/{desc}\"\n",
    "\n",
    "def tensors_dirs(desc: str) -> tuple[str, str]:\n",
    "    out_test   = f\"./tensors/{desc}_test\"\n",
    "    out_future = f\"./tensors/{desc}_future\"\n",
    "    os.makedirs(out_test, exist_ok=True)\n",
    "    os.makedirs(out_future, exist_ok=True)\n",
    "    return out_test, out_future\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Read thresholds\n",
    "# -----------------------------------------------------------------------------\n",
    "THRESH_CSV = Path(\"./youden_thresholds.csv\")\n",
    "if not THRESH_CSV.exists():\n",
    "    alt = Path(\"/home/hch/dementia/youden_thresholds.csv\")\n",
    "    if alt.exists(): THRESH_CSV = alt\n",
    "assert THRESH_CSV.exists(), f\"youden_thresholds.csv not found: {THRESH_CSV}\"\n",
    "\n",
    "thr_df = pd.read_csv(THRESH_CSV)\n",
    "for col in [\"arch\",\"ft\",\"ft_blks\",\"split\"]:\n",
    "    if col in thr_df.columns: thr_df[col] = thr_df[col].astype(str)\n",
    "\n",
    "thr_df = thr_df.query(\"split == 'valid'\").copy()\n",
    "\n",
    "def pick_threshold_row(arch: str, ft: str) -> pd.Series | None:\n",
    "    df = thr_df.query(\"arch == @arch and ft == @ft\").copy()\n",
    "    if ft == 'partial':\n",
    "        df = df.query(\"ft_blks == '4'\")\n",
    "    elif ft == 'lora':\n",
    "        df = df.query(\"ft_blks == 'full'\")\n",
    "    if len(df) == 0:\n",
    "        return None\n",
    "    if 'auc' in df.columns:\n",
    "        df = df.sort_values('auc', ascending=False)\n",
    "    return df.iloc[0]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Build & load model\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_model_for(arch: str, ft: str, img_size: int = IMG_SIZE, enable_amp: bool = True, lora_rank=None):\n",
    "    args = type(\"Args\", (), {})()\n",
    "    args.arch = arch; args.ft = ft; args.img_size = img_size\n",
    "    ftb = ft_blks_for(ft)\n",
    "    if ftb is not None: args.ft_blks = ftb\n",
    "    if ft == 'partial' and isinstance(getattr(args, 'ft_blks', None), str):\n",
    "        args.ft_blks = int(args.ft_blks)\n",
    "    if ft == 'lora':\n",
    "        try:\n",
    "            args.lora_rank = int(lora_rank) if lora_rank is not None and str(lora_rank) != 'nan' else 4\n",
    "        except Exception:\n",
    "            args.lora_rank = 4\n",
    "    return build_model(args)\n",
    "\n",
    "def safe_load_state_dict(model, sd):\n",
    "    if isinstance(sd, dict):\n",
    "        for key in [\"state_dict\", \"model\", \"net\", \"module\", \"ema_state\"]:\n",
    "            if key in sd and isinstance(sd[key], dict):\n",
    "                try:\n",
    "                    model.load_state_dict(sd[key], strict=False); return True\n",
    "                except Exception: pass\n",
    "        try:\n",
    "            model.load_state_dict(sd, strict=False); return True\n",
    "        except Exception: pass\n",
    "    return False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) ViT backbone & robust attention capture (HOOKS + FALLBACK with RoPE)\n",
    "# -----------------------------------------------------------------------------\n",
    "def find_vit_backbone(module: nn.Module):\n",
    "    if hasattr(module, \"blocks\"):\n",
    "        return module\n",
    "    for _, child in module.named_children():\n",
    "        vb = find_vit_backbone(child)\n",
    "        if vb is not None:\n",
    "            return vb\n",
    "    return None\n",
    "\n",
    "def patch_attention_to_capture(vit_module):\n",
    "    def _hook_attn_probs(mod, inputs, output):\n",
    "        A = None\n",
    "        if isinstance(inputs, (tuple, list)) and len(inputs) > 0 and torch.is_tensor(inputs[0]):\n",
    "            A = inputs[0]\n",
    "        if A is None and torch.is_tensor(output):\n",
    "            A = output\n",
    "        mod._last_attn = A\n",
    "        try:\n",
    "            if A is not None: A.retain_grad()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _fallback_hook(mod, inputs, output):\n",
    "        cand = None\n",
    "        cand_list = []\n",
    "        if isinstance(inputs, (tuple, list)): cand_list += list(inputs)\n",
    "        if torch.is_tensor(output): cand_list += [output]\n",
    "        for t in cand_list:\n",
    "            if torch.is_tensor(t) and t.ndim == 4:\n",
    "                cand = t; break\n",
    "        mod._last_attn = cand\n",
    "        try:\n",
    "            if cand is not None: cand.retain_grad()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    for blk in vit_module.blocks:\n",
    "        attn = blk.attn\n",
    "        if getattr(attn, \"_patched_capture\", False): continue\n",
    "\n",
    "        hooked = False\n",
    "        if hasattr(attn, \"attn_drop\") and isinstance(attn.attn_drop, nn.Module):\n",
    "            attn.attn_drop.register_forward_hook(_hook_attn_probs); hooked = True\n",
    "        if hasattr(attn, \"drop_attn\") and isinstance(attn.drop_attn, nn.Module):\n",
    "            attn.drop_attn.register_forward_hook(_hook_attn_probs); hooked = True\n",
    "        if hasattr(attn, \"softmax\") and isinstance(attn.softmax, nn.Module):\n",
    "            attn.softmax.register_forward_hook(_hook_attn_probs); hooked = True\n",
    "        if not hooked and hasattr(attn, \"proj_drop\") and isinstance(attn.proj_drop, nn.Module):\n",
    "            attn.proj_drop.register_forward_hook(_fallback_hook)\n",
    "\n",
    "        attn._patched_capture = True\n",
    "\n",
    "@torch.no_grad()\n",
    "def _relpos_bias_if_any(attn, N, device):\n",
    "    try:\n",
    "        if hasattr(attn, \"rel_pos_bias\") and attn.rel_pos_bias is not None:\n",
    "            BIAS = attn.rel_pos_bias()\n",
    "            if BIAS.dim()==3: BIAS = BIAS.unsqueeze(0)\n",
    "            return BIAS.to(device)\n",
    "        if hasattr(attn, \"rpe\") and attn.rpe is not None:\n",
    "            BIAS = attn.rpe()\n",
    "            if BIAS.dim()==3: BIAS = BIAS.unsqueeze(0)\n",
    "            return BIAS.to(device)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _rotate_half(x):\n",
    "    D = x.shape[-1]\n",
    "    D2 = D // 2\n",
    "    x1, x2 = x[..., :D2], x[..., D2:D2*2]\n",
    "    if x2.shape[-1] != x1.shape[-1]:\n",
    "        return x\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def _maybe_apply_rope(q, k, rope):\n",
    "    \"\"\"\n",
    "    Try several common patterns:\n",
    "      - callable(rope): returns (q', k')\n",
    "      - dict with {'cos', 'sin'}\n",
    "      - tuple/list (cos, sin)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if rope is None:\n",
    "            return q, k\n",
    "        if callable(rope):\n",
    "            out = rope(q, k)\n",
    "            if isinstance(out, (tuple, list)) and len(out) == 2 and torch.is_tensor(out[0]) and torch.is_tensor(out[1]):\n",
    "                return out[0], out[1]\n",
    "        if isinstance(rope, dict):\n",
    "            cos = rope.get('cos', None); sin = rope.get('sin', None)\n",
    "            if torch.is_tensor(cos) and torch.is_tensor(sin):\n",
    "                while cos.ndim < q.ndim: cos = cos.unsqueeze(0)\n",
    "                while sin.ndim < q.ndim: sin = sin.unsqueeze(0)\n",
    "                q = q * cos + _rotate_half(q) * sin\n",
    "                k = k * cos + _rotate_half(k) * sin\n",
    "                return q, k\n",
    "        if isinstance(rope, (tuple, list)) and len(rope) == 2:\n",
    "            cos, sin = rope\n",
    "            if torch.is_tensor(cos) and torch.is_tensor(sin):\n",
    "                while cos.ndim < q.ndim: cos = cos.unsqueeze(0)\n",
    "                while sin.ndim < q.ndim: sin = sin.unsqueeze(0)\n",
    "                q = q * cos + _rotate_half(q) * sin\n",
    "                k = k * cos + _rotate_half(k) * sin\n",
    "                return q, k\n",
    "    except Exception:\n",
    "        pass\n",
    "    return q, k\n",
    "\n",
    "def fallback_patch_attention_override(vit_module):\n",
    "    for blk in vit_module.blocks:\n",
    "        attn = blk.attn\n",
    "        if getattr(attn, \"_override_forward\", False):\n",
    "            continue\n",
    "        if not (hasattr(attn, \"qkv\") and hasattr(attn, \"proj\")):\n",
    "            continue\n",
    "\n",
    "        def wrapped_forward(x, *args, **kwargs):\n",
    "            rope = kwargs.pop(\"rope\", None)  # DINOv3 passes rope=...\n",
    "            B, N, C = x.shape\n",
    "            qkv = attn.qkv(x)                                # [B,N,3*C]\n",
    "            head_dim = C // attn.num_heads\n",
    "            qkv = qkv.reshape(B, N, 3, attn.num_heads, head_dim).permute(2,0,3,1,4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]                  # [B,H,N,D]\n",
    "\n",
    "            if hasattr(attn, \"q_norm\") and attn.q_norm is not None:\n",
    "                q = attn.q_norm(q.transpose(2,3)).transpose(2,3)\n",
    "            if hasattr(attn, \"k_norm\") and attn.k_norm is not None:\n",
    "                k = attn.k_norm(k.transpose(2,3)).transpose(2,3)\n",
    "\n",
    "            q, k = _maybe_apply_rope(q, k, rope)\n",
    "\n",
    "            scale = (q.shape[-1] ** -0.5)\n",
    "            q = q * scale\n",
    "            attn_scores = torch.matmul(q, k.transpose(-2, -1))  # [B,H,N,N]\n",
    "\n",
    "            BIAS = _relpos_bias_if_any(attn, N, x.device)\n",
    "            if BIAS is not None:\n",
    "                attn_scores = attn_scores + (BIAS if BIAS.shape[0]==1 else BIAS[:B])\n",
    "\n",
    "            A = attn_scores.softmax(dim=-1)\n",
    "            attn._last_attn = A\n",
    "            try: A.retain_grad()\n",
    "            except Exception: pass\n",
    "\n",
    "            if hasattr(attn, \"attn_drop\") and attn.attn_drop is not None:\n",
    "                A = attn.attn_drop(A)\n",
    "\n",
    "            x_out = torch.matmul(A, v)\n",
    "            x_out = x_out.transpose(1,2).reshape(B, N, C)\n",
    "            x_out = attn.proj(x_out)\n",
    "            if hasattr(attn, \"proj_drop\") and attn.proj_drop is not None:\n",
    "                x_out = attn.proj_drop(x_out)\n",
    "            return x_out\n",
    "\n",
    "        attn.forward = wrapped_forward\n",
    "        attn._override_forward = True\n",
    "\n",
    "def _has_any_attn(vit_module):\n",
    "    for blk in vit_module.blocks:\n",
    "        attn = blk.attn\n",
    "        for attr in [\"attn_drop\",\"drop_attn\",\"softmax\",\"proj_drop\"]:\n",
    "            if hasattr(attn, attr):\n",
    "                m = getattr(attn, attr)\n",
    "                if isinstance(m, nn.Module) and getattr(m, \"_last_attn\", None) is not None:\n",
    "                    return True\n",
    "        if getattr(attn, \"_last_attn\", None) is not None:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def ensure_attention_capture(model, vit_module, sample_tensor):\n",
    "    sample_tensor = sample_tensor.clone().detach().requires_grad_(True)\n",
    "\n",
    "    # try with hooks\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    out = model(sample_tensor)\n",
    "    if isinstance(out, dict): out = out.get(\"logits\", out.get(\"pred\", out))\n",
    "    if not torch.is_tensor(out): out = torch.as_tensor(out)\n",
    "    out.flatten()[0].backward(retain_graph=True)\n",
    "\n",
    "    if _has_any_attn(vit_module):\n",
    "        return\n",
    "\n",
    "    # fallback override (RoPE-aware)\n",
    "    fallback_patch_attention_override(vit_module)\n",
    "\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    out = model(sample_tensor)\n",
    "    if isinstance(out, dict): out = out.get(\"logits\", out.get(\"pred\", out))\n",
    "    if not torch.is_tensor(out): out = torch.as_tensor(out)\n",
    "    out.flatten()[0].backward(retain_graph=True)\n",
    "\n",
    "    if not _has_any_attn(vit_module):\n",
    "        raise RuntimeError(\"Attention capture failed (hooks & override). Check this architecture.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Grad-CAM rollout (non-square token grid safe)\n",
    "# -----------------------------------------------------------------------------\n",
    "class ViTGradCamRollout:\n",
    "    def __init__(self, vit_module):\n",
    "        self.vit = vit_module\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_attn(M):\n",
    "        M = M / (M.sum(dim=-1, keepdim=True) + 1e-6)\n",
    "        T = M.size(-1)\n",
    "        M = M + torch.eye(T, device=M.device, dtype=M.dtype)\n",
    "        M = M / (M.sum(dim=-1, keepdim=True) + 1e-6)\n",
    "        return M\n",
    "\n",
    "    def compute_cam(self, model, img_tensor, class_idx=1):\n",
    "        img_tensor = img_tensor.clone().detach().requires_grad_(True)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        out = model(img_tensor)\n",
    "        if isinstance(out, dict):\n",
    "            out = out.get(\"logits\", out.get(\"pred\", out))\n",
    "        if not torch.is_tensor(out):\n",
    "            out = torch.as_tensor(out)\n",
    "\n",
    "        if out.ndim == 1:\n",
    "            score = out[0]\n",
    "        elif out.ndim == 2 and out.shape[1] == 1:\n",
    "            score = out[0, 0]\n",
    "        elif out.ndim == 2 and out.shape[1] >= 2:\n",
    "            score = out[0, class_idx]\n",
    "        else:\n",
    "            score = out.reshape(-1)[0]\n",
    "        score.backward(retain_graph=False)\n",
    "\n",
    "        attn_list, grad_list = [], []\n",
    "        for blk in self.vit.blocks:\n",
    "            attn = blk.attn\n",
    "            A, G = None, None\n",
    "            for attr in [\"attn_drop\", \"drop_attn\", \"softmax\", \"proj_drop\"]:\n",
    "                if hasattr(attn, attr):\n",
    "                    m = getattr(attn, attr)\n",
    "                    if isinstance(m, nn.Module):\n",
    "                        A = getattr(m, \"_last_attn\", None)\n",
    "                        if A is not None:\n",
    "                            G = getattr(A, \"grad\", None)\n",
    "                            break\n",
    "            if A is None:\n",
    "                A = getattr(attn, \"_last_attn\", None)\n",
    "                if A is not None:\n",
    "                    G = getattr(A, \"grad\", None)\n",
    "            if A is None:\n",
    "                continue\n",
    "            attn_list.append(A); grad_list.append(G)\n",
    "\n",
    "        if len(attn_list) == 0:\n",
    "            raise RuntimeError(\"No attention captured; check hook patching for this architecture.\")\n",
    "\n",
    "        # If grid size known, use it; else infer\n",
    "        if hasattr(self.vit, \"patch_embed\") and hasattr(self.vit.patch_embed, \"grid_size\"):\n",
    "            gh, gw = self.vit.patch_embed.grid_size\n",
    "        else:\n",
    "            Ttok = attn_list[0].shape[-1]\n",
    "            n = Ttok - 1  # exclude CLS\n",
    "            s = int(round(math.sqrt(n)))  # allow non-square\n",
    "            gh = gw = s\n",
    "\n",
    "        rollout = None\n",
    "        for A, G in zip(attn_list, grad_list):\n",
    "            A = A[0].mean(dim=0).mean(dim=0)  # [T,T]\n",
    "            if G is not None:\n",
    "                G = G[0].mean(dim=0).mean(dim=0)\n",
    "                M = (A * G).clamp(min=0)\n",
    "            else:\n",
    "                M = A.clamp(min=0)\n",
    "            M = self._normalize_attn(M)\n",
    "            rollout = M if rollout is None else torch.matmul(rollout, M)\n",
    "\n",
    "        cls_to_patches = rollout[0, 1:].detach().cpu().numpy()\n",
    "        n = cls_to_patches.size\n",
    "        s = int(round(math.sqrt(n)))\n",
    "        gh = gw = s\n",
    "\n",
    "        # pad or trim to s*s\n",
    "        if n < s*s:\n",
    "            pad = np.zeros((s*s,), dtype=cls_to_patches.dtype)\n",
    "            pad[:n] = cls_to_patches\n",
    "            cls_to_patches = pad\n",
    "        elif n > s*s:\n",
    "            cls_to_patches = cls_to_patches[:s*s]\n",
    "\n",
    "        cam = cls_to_patches.reshape(gh, gw)\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-6)\n",
    "        return cam\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Dataset + meta join & TP filtering\n",
    "# -----------------------------------------------------------------------------\n",
    "def try_get_df_row(ds, idx):\n",
    "    for attr in [\"df\", \"meta\", \"ann\", \"items\", \"data\"]:\n",
    "        if hasattr(ds, attr):\n",
    "            obj = getattr(ds, attr)\n",
    "            if hasattr(obj, \"iloc\"): return obj.iloc[idx]\n",
    "            if isinstance(obj, (list, tuple)) and len(obj) > idx: return obj[idx]\n",
    "    return None\n",
    "\n",
    "def extract_path(row):\n",
    "    for c in [\"pngfilename\", \"img_path\", \"filepath\", \"path\"]:\n",
    "        if row is not None and (c in row): return row[c]\n",
    "    return None\n",
    "\n",
    "def attach_imgpath(df_csv, ds):\n",
    "    rows = []\n",
    "    for _, r in df_csv.iterrows():\n",
    "        meta = try_get_df_row(ds, int(r[\"idx\"]))\n",
    "        img = extract_path(meta)\n",
    "        rows.append({**r.to_dict(), \"img_path\": img})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def normalize_master(master_csv: str = MASTER_CSV) -> pd.DataFrame:\n",
    "    ddd = pd.read_csv(master_csv)\n",
    "    ddd['pngfilename'] = ddd['pngfilename'].str.replace('.', '_', regex=False)\n",
    "    ddd['pngfilename'] = ddd['pngfilename'].str.replace('_png', '.png', regex=False)\n",
    "    ddd['img_path'] = '/home/hch/opportunistic/AutoMorph_Data/Results/M0/images/' + ddd['pngfilename']\n",
    "    return ddd[['img_path','eye_orientation','pngfilename']]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Export tensors (origin_images.pth & saliency_map.pth)\n",
    "# -----------------------------------------------------------------------------\n",
    "preprocess = Compose([Resize((IMG_SIZE, IMG_SIZE)), ToTensor()])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_tensor_01(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    t = preprocess(img)  # [3,H,W], 0..1\n",
    "    return t\n",
    "\n",
    "def ensure_path(p):\n",
    "    if p and os.path.isabs(p):\n",
    "        return p\n",
    "    return os.path.join(WORK_DIR, p) if isinstance(p, str) else None\n",
    "\n",
    "def compute_cam_tensor(rollout, model, img_path):\n",
    "    full = ensure_path(img_path)\n",
    "    if (not full) or (not os.path.exists(full)): return None, None\n",
    "    x = load_tensor_01(full).unsqueeze(0).to(device, non_blocking=True)  # [1,3,H,W]\n",
    "    cam_hw = rollout.compute_cam(model, x, class_idx=1)                  # [h',w'] in [0,1]\n",
    "    cam_hw = cv2.resize((cam_hw*255).astype(np.uint8), (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_CUBIC)\n",
    "    cam_hw = torch.from_numpy(cam_hw).float().div(255.0)                 # [H,W] in [0,1]\n",
    "    return x.squeeze(0).cpu(), cam_hw.unsqueeze(0)                       # [3,H,W], [1,H,W]\n",
    "\n",
    "def export_tp_pack(tp_df: pd.DataFrame, out_dir: str, rollout: ViTGradCamRollout, model: nn.Module):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ori_list, cam_list, meta_list = [], [], []\n",
    "    for _, row in tqdm(tp_df.iterrows(), total=len(tp_df), desc=f\"Export {Path(out_dir).name}\"):\n",
    "        p = row[\"img_path\"]\n",
    "        ori, cam = compute_cam_tensor(rollout, model, p)\n",
    "        if ori is None: continue\n",
    "        ori_list.append(ori.unsqueeze(0))  # [1,3,H,W]\n",
    "        cam_list.append(cam.unsqueeze(0))  # [1,1,H,W]\n",
    "        meta_list.append({\n",
    "            \"idx\": int(row[\"idx\"]),\n",
    "            \"img_path\": ensure_path(p),\n",
    "            \"eye_orientation\": row.get(\"eye_orientation\", None),\n",
    "            \"pred\": float(row[\"pred\"]),\n",
    "            \"label_or_event\": int(row.get(\"label\", row.get(\"event\", -1)))\n",
    "        })\n",
    "    if len(ori_list) == 0:\n",
    "        print(\"No TP images to export:\", out_dir); return False\n",
    "    origin_images = torch.cat(ori_list, dim=0)        # [N,3,H,W]\n",
    "    saliency_map  = torch.cat(cam_list, dim=0)        # [N,1,H,W]\n",
    "    torch.save(origin_images.contiguous(), os.path.join(out_dir, \"origin_images.pth\"))\n",
    "    torch.save(saliency_map.contiguous(),  os.path.join(out_dir, \"saliency_map.pth\"))\n",
    "    with open(os.path.join(out_dir, \"meta.json\"), \"w\") as f:\n",
    "        json.dump(meta_list, f, indent=2)\n",
    "    print(f\"[✓] Saved: {out_dir}/origin_images.pth, saliency_map.pth  (N={len(meta_list)})\")\n",
    "    return True\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8) Rep Grad-CAM L/R overlay (FOV-safe)\n",
    "# -----------------------------------------------------------------------------\n",
    "REP_ROOT = Path(\"./rep_gradcam_lr\")\n",
    "REP_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "def make_fov_mask(h=IMG_SIZE, w=IMG_SIZE) -> torch.Tensor:\n",
    "    yy, xx = np.ogrid[:h, :w]\n",
    "    cy, cx = h // 2, w // 2\n",
    "    r = min(h, w) // 2\n",
    "    fov = (yy - cy)**2 + (xx - cx)**2 <= (r**2)\n",
    "    return torch.from_numpy(fov.astype(np.uint8))\n",
    "\n",
    "def _to_numpy_img(t3chw: torch.Tensor):\n",
    "    return t3chw.detach().cpu().clamp(0,1).permute(1,2,0).numpy()\n",
    "\n",
    "def _overlay_cam(rgb_hw3, cam_hw, alpha=0.45, cmap_name=\"jet\"):\n",
    "    cmap = plt.get_cmap(cmap_name)\n",
    "    cam_rgb = cmap(cam_hw)[..., :3]\n",
    "    out = (1 - alpha) * rgb_hw3 + alpha * cam_rgb\n",
    "    return np.clip(out, 0, 1)\n",
    "\n",
    "def _norm_eye_label(x):\n",
    "    if x is None: return None\n",
    "    s = str(x).strip().lower()\n",
    "    if s in [\"r\",\"right\",\"rt\",\"오\",\"오른\",\"오른쪽\",\"1\",\"true\",\"od\",\"o.d\"]: return \"R\"\n",
    "    if s in [\"l\",\"left\",\"lt\",\"왼\",\"왼쪽\",\"0\",\"false\",\"os\",\"o.s\"]: return \"L\"\n",
    "    try: return \"R\" if int(float(s)) == 1 else \"L\"\n",
    "    except: return None\n",
    "\n",
    "def _load_pack(src_dir: str):\n",
    "    p = Path(src_dir)\n",
    "    ori = torch.load(p/\"origin_images.pth\")\n",
    "    cam = torch.load(p/\"saliency_map.pth\")\n",
    "    with open(p/\"meta.json\",\"r\") as f:\n",
    "        metas = json.load(f)\n",
    "    return ori, cam, metas\n",
    "\n",
    "def _indices_by_eye(metas, eye_code):\n",
    "    return [i for i,m in enumerate(metas) if _norm_eye_label(m.get(\"eye_orientation\")) == eye_code]\n",
    "\n",
    "def _avg_heatmap(cam: torch.Tensor, indices, apply_fov=True):\n",
    "    if not indices: return None\n",
    "    arr = cam[indices,0].detach().cpu().numpy()\n",
    "    mean = arr.mean(axis=0)\n",
    "    if apply_fov:\n",
    "        fov = make_fov_mask().numpy().astype(bool)\n",
    "        mean = mean * fov\n",
    "        valid = fov.any()\n",
    "        mn = mean[fov].min() if valid else mean.min()\n",
    "        mx = mean[fov].max() if valid else mean.max()\n",
    "    else:\n",
    "        mn, mx = mean.min(), mean.max()\n",
    "    if mx > mn: mean = (mean - mn)/(mx - mn + 1e-8)\n",
    "    return mean\n",
    "\n",
    "def _random_from(indices, seed=None):\n",
    "    if not indices: return None\n",
    "    rng = random.Random(seed); return rng.choice(indices)\n",
    "\n",
    "def _save_img(array_hw3, fname, out_dir: Path):\n",
    "    out_dir.mkdir(exist_ok=True, parents=True)\n",
    "    plt.imsave(out_dir / fname, array_hw3); print(f\"[✓] Saved: {out_dir/fname}\")\n",
    "\n",
    "def _save_heatmap(array_hw, fname, out_dir: Path, cmap=\"jet\"):\n",
    "    out_dir.mkdir(exist_ok=True, parents=True)\n",
    "    plt.imsave(out_dir / fname, array_hw, cmap=cmap); print(f\"[✓] Saved: {out_dir/fname}\")\n",
    "\n",
    "def save_heatmap_and_overlay(src_dir: str, model_desc: str, set_name=\"TEST\", seed=0, cmap=\"jet\"):\n",
    "    ori, cam, metas = _load_pack(src_dir)\n",
    "    idxs_L = _indices_by_eye(metas, \"L\")\n",
    "    idxs_R = _indices_by_eye(metas, \"R\")\n",
    "    avg_L = _avg_heatmap(cam, idxs_L, apply_fov=True)\n",
    "    avg_R = _avg_heatmap(cam, idxs_R, apply_fov=True)\n",
    "    rL = _random_from(idxs_L, seed)\n",
    "    rR = _random_from(idxs_R, seed+1)\n",
    "\n",
    "    out_dir = REP_ROOT / model_desc\n",
    "    if avg_L is not None:\n",
    "        _save_heatmap(avg_L, f\"{set_name}_Left_heatmap.png\", out_dir, cmap)\n",
    "        if rL is not None:\n",
    "            overlay_L = _overlay_cam(_to_numpy_img(ori[rL]), avg_L, alpha=0.45, cmap_name=cmap)\n",
    "            _save_img(overlay_L, f\"{set_name}_Left_overlay.png\", out_dir)\n",
    "    if avg_R is not None:\n",
    "        _save_heatmap(avg_R, f\"{set_name}_Right_heatmap.png\", out_dir, cmap)\n",
    "        if rR is not None:\n",
    "            overlay_R = _overlay_cam(_to_numpy_img(ori[rR]), avg_R, alpha=0.45, cmap_name=cmap)\n",
    "            _save_img(overlay_R, f\"{set_name}_Right_overlay.png\", out_dir)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 9) Mask utilities + attention summaries (ODC split into 4 wedges)\n",
    "# -----------------------------------------------------------------------------\n",
    "def ensure_bool_mask(mask_img: Image.Image, size=(IMG_SIZE, IMG_SIZE)) -> torch.Tensor:\n",
    "    if not isinstance(mask_img, Image.Image):\n",
    "        mask_img = Image.fromarray(np.asarray(mask_img))\n",
    "    mask = mask_img.resize(size, resample=Image.NEAREST)\n",
    "    mask_np = np.array(mask)\n",
    "    mask_bin = (mask_np > 0).astype(np.uint8)\n",
    "    return torch.from_numpy(mask_bin)\n",
    "\n",
    "def load_mask_by_filename(root: Path, pngfilename: str) -> torch.Tensor | None:\n",
    "    cand = root / pngfilename\n",
    "    if cand.exists():\n",
    "        return ensure_bool_mask(Image.open(cand))\n",
    "    stem = Path(pngfilename).stem\n",
    "    for ext in (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"):\n",
    "        p = root / f\"{stem}{ext}\"\n",
    "        if p.exists():\n",
    "            return ensure_bool_mask(Image.open(p))\n",
    "    return None\n",
    "\n",
    "def load_optic_disc_cup_union(pngfilename: str) -> torch.Tensor | None:\n",
    "    p = ODC_RAW_DIR / pngfilename\n",
    "    if not p.exists():\n",
    "        stem = Path(pngfilename).stem\n",
    "        found = None\n",
    "        for ext in (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"):\n",
    "            cand = ODC_RAW_DIR / f\"{stem}{ext}\"\n",
    "            if cand.exists():\n",
    "                found = cand; break\n",
    "        if found is None:\n",
    "            return None\n",
    "        p = found\n",
    "    try:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        arr = np.array(img)\n",
    "        r = arr[..., 0]\n",
    "        b = arr[..., 2]\n",
    "        rb_union = ((r > 0) | (b > 0)).astype(np.uint8)\n",
    "        return ensure_bool_mask(Image.fromarray(rb_union * 255))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def split_odc_quadrants(odc: torch.Tensor):\n",
    "    if odc is None or odc.ndim != 2:\n",
    "        return None\n",
    "    h, w = odc.shape\n",
    "    ys, xs = torch.nonzero(odc > 0, as_tuple=True)\n",
    "    if ys.numel() == 0:\n",
    "        return None\n",
    "\n",
    "    cy = ys.float().mean()\n",
    "    cx = xs.float().mean()\n",
    "\n",
    "    yy = torch.arange(h, device=odc.device).view(-1, 1).expand(h, w).float()\n",
    "    xx = torch.arange(w, device=odc.device).view(1, -1).expand(h, w).float()\n",
    "\n",
    "    sign1 = (yy - cy) - (xx - cx)\n",
    "    sign2 = (yy - cy) + (xx - cx)\n",
    "\n",
    "    superior = ((sign1 < 0) & (sign2 < 0) & (odc > 0)).to(torch.uint8)\n",
    "    inferior = ((sign1 > 0) & (sign2 > 0) & (odc > 0)).to(torch.uint8)\n",
    "    right_w  = ((sign1 < 0) & (sign2 > 0) & (odc > 0)).to(torch.uint8)\n",
    "    left_w   = ((sign1 > 0) & (sign2 < 0) & (odc > 0)).to(torch.uint8)\n",
    "\n",
    "    return {\n",
    "        \"superior\": superior,\n",
    "        \"inferior\": inferior,\n",
    "        \"right\": right_w,\n",
    "        \"left\": left_w,\n",
    "        \"cx\": float(cx.item()),\n",
    "        \"cy\": float(cy.item()),\n",
    "    }\n",
    "\n",
    "def normalize_gradcam_into_fov(gradcam_t: torch.Tensor, fov_mask: torch.Tensor) -> torch.Tensor | None:\n",
    "    if gradcam_t.dim() == 3:\n",
    "        if gradcam_t.size(0) in (1,3):\n",
    "            gradcam_t = gradcam_t[0, ...] if gradcam_t.size(0) > 0 else gradcam_t.squeeze(0)\n",
    "        else:\n",
    "            gradcam_t = gradcam_t.squeeze(0)\n",
    "    elif gradcam_t.dim() == 4:\n",
    "        gradcam_t = gradcam_t.squeeze()\n",
    "    gc = TF.to_pil_image(gradcam_t.float())\n",
    "    gc = gc.resize((IMG_SIZE, IMG_SIZE), resample=Image.BILINEAR)\n",
    "    gc = TF.pil_to_tensor(gc).squeeze().float()\n",
    "    gc = gc * fov_mask\n",
    "    s = gc.sum().item()\n",
    "    if s <= 0:\n",
    "        return None\n",
    "    gc = 100.0 * gc / s\n",
    "    return gc\n",
    "\n",
    "def region_mean(gc_norm100: torch.Tensor, region_mask01: torch.Tensor) -> float:\n",
    "    denom = region_mask01.sum().item()\n",
    "    if denom <= 0: return float(\"nan\")\n",
    "    return (gc_norm100 * region_mask01).sum().item() / denom\n",
    "\n",
    "def _safe_ratio(num: pd.Series, den: pd.Series) -> pd.Series:\n",
    "    num = pd.to_numeric(num, errors=\"coerce\"); den = pd.to_numeric(den, errors=\"coerce\")\n",
    "    ratio = num / den\n",
    "    ratio[(~np.isfinite(den)) | (den <= 0)] = np.nan\n",
    "    return ratio\n",
    "\n",
    "def bootstrap_ci(data: np.ndarray, n_boot: int = 2000, ci: int = 95, seed: int = 42):\n",
    "    data = np.asarray(data); data = data[np.isfinite(data)]\n",
    "    if data.size == 0: return (np.nan, np.nan)\n",
    "    rng = np.random.default_rng(seed); means = np.empty(n_boot); n = data.size\n",
    "    for i in range(n_boot):\n",
    "        sample = rng.choice(data, size=n, replace=True)\n",
    "        means[i] = np.nanmean(sample)\n",
    "    alpha = (100 - ci) / 2.0\n",
    "    lower = np.percentile(means, alpha); upper = np.percentile(means, 100 - alpha)\n",
    "    return (lower, upper)\n",
    "\n",
    "def summarize_attention(res_df: pd.DataFrame, label: str = \"TEST\", n_boot: int = 2000) -> pd.DataFrame:\n",
    "    ref = res_df[\"not_optic_disc_not_both_n\"]\n",
    "    ratios = pd.DataFrame({\n",
    "        \"vascular_attention_vein\":   _safe_ratio(res_df[\"veins_n\"],        ref),\n",
    "        \"vascular_attention_artery\": _safe_ratio(res_df[\"arteries_n\"],      ref),\n",
    "        \"vascular_attention_both\":   _safe_ratio(res_df[\"both_n\"],          ref),\n",
    "        \"optic_disc_attention\":      _safe_ratio(res_df[\"optic_disc_n\"],    ref),\n",
    "        \"optic_disc_attention_temporal\": _safe_ratio(res_df[\"optic_disc_temporal_n\"], ref),\n",
    "        \"optic_disc_attention_nasal\":    _safe_ratio(res_df[\"optic_disc_nasal_n\"],     ref),\n",
    "        \"optic_disc_attention_superior\": _safe_ratio(res_df[\"optic_disc_superior_n\"],  ref),\n",
    "        \"optic_disc_attention_inferior\": _safe_ratio(res_df[\"optic_disc_inferior_n\"],  ref),\n",
    "    })\n",
    "    mean_vals = ratios.mean(numeric_only=True)\n",
    "    results = {}\n",
    "    for col in ratios.columns:\n",
    "        ci_low, ci_high = bootstrap_ci(ratios[col].values, n_boot=n_boot, ci=95)\n",
    "        results[col] = {\n",
    "            \"mean\": float(mean_vals[col]),\n",
    "            \"95%_CI_low\": float(ci_low),\n",
    "            \"95%_CI_high\": float(ci_high),\n",
    "            \"n_used\": int(np.isfinite(ratios[col].values).sum())\n",
    "        }\n",
    "    out = pd.DataFrame(results).T\n",
    "    print(f\"\\n=== [{label}] Attention ratios (ref = not_optic_disc_not_both_n) ===\")\n",
    "    print(mean_vals.to_string())\n",
    "    return ratios, out\n",
    "\n",
    "def compute_metrics_for_dataset(df_csv: Path, saliency_pth: Path, out_dir: Path) -> pd.DataFrame:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df = pd.read_csv(df_csv)\n",
    "    if \"pngfilename\" not in df.columns:\n",
    "        raise ValueError(f\"'pngfilename' column not found in {df_csv}\")\n",
    "    gradcams = torch.load(saliency_pth)\n",
    "    n = len(gradcams) if hasattr(gradcams, \"__len__\") else int(gradcams.size(0))\n",
    "    df = df.reset_index(drop=True)\n",
    "    if len(df) != n:\n",
    "        m = min(len(df), n)\n",
    "        df = df.iloc[:m].reset_index(drop=True)\n",
    "        if hasattr(gradcams, \"__len__\"):\n",
    "            gradcams = gradcams[:m]\n",
    "        else:\n",
    "            gradcams = gradcams[:m, ...]\n",
    "    records = []\n",
    "    fov = make_fov_mask()\n",
    "    for idx in tqdm(range(len(df)), desc=f\"Computing masks for {df_csv.stem}\", total=len(df)):\n",
    "        row = df.iloc[idx]\n",
    "        pngfilename = str(row[\"pngfilename\"])\n",
    "        artery = load_mask_by_filename(ARTERY_DIR, pngfilename)\n",
    "        vein   = load_mask_by_filename(VEIN_DIR,   pngfilename)\n",
    "        if artery is None or vein is None:\n",
    "            continue\n",
    "        odc = load_optic_disc_cup_union(pngfilename)  # may be None\n",
    "        both = (artery | vein).clamp_(0, 1)\n",
    "        not_both = ((1 - both) * fov).clamp_(0, 1)\n",
    "        if odc is not None:\n",
    "            odc = (odc * fov).clamp_(0, 1)\n",
    "            not_odc = ((1 - odc) * fov).clamp_(0, 1)\n",
    "            not_odc_not_both = ((1 - ((both | odc).clamp_(0, 1))) * fov).clamp_(0, 1)\n",
    "            # --- ODC를 X자 기준으로 4개 쐐기 분할\n",
    "            quads = split_odc_quadrants(odc)\n",
    "        else:\n",
    "            not_odc = None; not_odc_not_both = None; quads = None\n",
    "\n",
    "        gc = normalize_gradcam_into_fov(gradcams[idx], fov)\n",
    "        if gc is None:\n",
    "            continue\n",
    "\n",
    "        vein_ = (vein * fov).clamp_(0, 1);    not_vein = ((1 - vein) * fov).clamp_(0, 1)\n",
    "        artery_ = (artery * fov).clamp_(0, 1);not_artery = ((1 - artery) * fov).clamp_(0, 1)\n",
    "        both_   = (both * fov).clamp_(0, 1);  not_both_ = not_both\n",
    "\n",
    "        eye_code = _norm_eye_label(row.get(\"eye_orientation\", None))\n",
    "\n",
    "        od_temporal = float(\"nan\")\n",
    "        od_nasal    = float(\"nan\")\n",
    "        od_sup      = float(\"nan\")\n",
    "        od_inf      = float(\"nan\")\n",
    "\n",
    "        if (gc is not None) and (quads is not None):\n",
    "\n",
    "            od_sup = region_mean(gc, quads[\"superior\"])\n",
    "            od_inf = region_mean(gc, quads[\"inferior\"])\n",
    "\n",
    "            if eye_code == \"L\":\n",
    "                temporal_mask = quads[\"right\"]\n",
    "                nasal_mask    = quads[\"left\"]\n",
    "            elif eye_code == \"R\":\n",
    "                temporal_mask = quads[\"left\"]\n",
    "                nasal_mask    = quads[\"right\"]\n",
    "            else:\n",
    "                temporal_mask = None\n",
    "                nasal_mask    = None\n",
    "\n",
    "            if temporal_mask is not None:\n",
    "                od_temporal = region_mean(gc, temporal_mask)\n",
    "            if nasal_mask is not None:\n",
    "                od_nasal = region_mean(gc, nasal_mask)\n",
    "\n",
    "        result = {\n",
    "            \"pngfilename\": pngfilename,\n",
    "            \"veins_n\":               region_mean(gc, vein_),\n",
    "            \"not_veins_n\":           region_mean(gc, not_vein),\n",
    "            \"arteries_n\":            region_mean(gc, artery_),\n",
    "            \"not_arteries_n\":        region_mean(gc, not_artery),\n",
    "            \"both_n\":                region_mean(gc, both_),\n",
    "            \"not_both_n\":            region_mean(gc, not_both_),\n",
    "            \"has_optic_disc\":        bool(odc is not None),\n",
    "            \"optic_disc_n\":          float(\"nan\") if odc is None else region_mean(gc, odc),\n",
    "            \"not_optic_disc_n\":      float(\"nan\") if odc is None else region_mean(gc, not_odc),\n",
    "            \"not_optic_disc_not_both_n\": float(\"nan\") if odc is None else region_mean(gc, not_odc_not_both),\n",
    "\n",
    "            \"optic_disc_temporal_n\": od_temporal,\n",
    "            \"optic_disc_nasal_n\":    od_nasal,\n",
    "            \"optic_disc_superior_n\": od_sup,\n",
    "            \"optic_disc_inferior_n\": od_inf,\n",
    "        }\n",
    "        records.append(result)\n",
    "    res = pd.DataFrame.from_records(records)\n",
    "    out_csv = out_dir / f\"RESULTS_{df_csv.stem.replace('_df','')}.csv\"\n",
    "    res.to_csv(out_csv, index=False)\n",
    "    print(f\"[Saved] {out_csv}  (n={len(res)})\")\n",
    "    if len(res):\n",
    "        cols = [\"veins_n\",\"not_veins_n\",\"arteries_n\",\"not_arteries_n\",\"both_n\",\"not_both_n\",\n",
    "                \"optic_disc_n\",\"not_optic_disc_n\",\"not_optic_disc_not_both_n\",\n",
    "                \"optic_disc_temporal_n\",\"optic_disc_nasal_n\",\"optic_disc_superior_n\",\"optic_disc_inferior_n\"]\n",
    "        summary = res[cols].mean(numeric_only=True)\n",
    "        print(\"Column means:\\n\", summary.to_string())\n",
    "    return res\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 10) MAIN LOOP over all combos\n",
    "# -----------------------------------------------------------------------------\n",
    "preprocess = Compose([Resize((IMG_SIZE, IMG_SIZE)), ToTensor()])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "master_norm = normalize_master(MASTER_CSV)\n",
    "\n",
    "skip_n = 0\n",
    "count = 0\n",
    "\n",
    "for arch in ARCHES:\n",
    "    for ft in FTS:\n",
    "        if count < skip_n:\n",
    "            print(f\"[SKIP {count+1}/{skip_n}] arch={arch}, ft={ft}\")\n",
    "            count += 1\n",
    "            continue\n",
    "        \n",
    "        row = pick_threshold_row(arch, ft)\n",
    "        if row is None:\n",
    "            print(f\"[SKIP] No threshold row in youden_thresholds.csv for: arch={arch}, ft={ft}\")\n",
    "            continue\n",
    "\n",
    "        desc = model_desc_from_csv_row(row)\n",
    "\n",
    "        if ft == 'partial' and str(row['ft_blks']) != '4':\n",
    "            print(f\"[SKIP] Found threshold with ft_blks={row['ft_blks']} but need 4: {arch}|{ft}\")\n",
    "            continue\n",
    "        if ft == 'lora' and str(row['ft_blks']) != 'full':\n",
    "            print(f\"[SKIP] Found threshold with ft_blks={row['ft_blks']} but need full: {arch}|{ft}\")\n",
    "            continue\n",
    "\n",
    "        BEST_THRESH = float(row['youden_thr'])\n",
    "        AUC_USED    = float(row.get('auc', float('nan')))\n",
    "        LORA_RANK   = row.get('lora_rank', None)\n",
    "\n",
    "        ckpt = ckpt_path(desc)\n",
    "        infer = infer_dir(desc)\n",
    "        OUT_ROOT_TEST, OUT_ROOT_FUTURE = tensors_dirs(desc)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"[{desc}]  arch={arch}  ft={ft}  ft_blks={row['ft_blks']}  lora_rank={LORA_RANK}  AUC={AUC_USED:.3f}\")\n",
    "        print(f\"  THRESH={BEST_THRESH:.6f}\")\n",
    "        print(f\"  CKPT  : {ckpt}\")\n",
    "        print(f\"  INFER : {infer}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # ---- Build & load model\n",
    "        model = build_model_for(arch, ft, IMG_SIZE, enable_amp=True, lora_rank=LORA_RANK).to(device).eval()\n",
    "        sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "        ok = safe_load_state_dict(model, sd)\n",
    "        if not ok:\n",
    "            print(f\"[SKIP] Failed to load weights from: {ckpt}\")\n",
    "            continue\n",
    "\n",
    "        # ---- Patch attention & rollout\n",
    "        vit = find_vit_backbone(model)\n",
    "        if vit is None:\n",
    "            print(f\"[SKIP] ViT backbone with 'blocks' not found in model: {desc}\")\n",
    "            continue\n",
    "        patch_attention_to_capture(vit)\n",
    "        rollout = ViTGradCamRollout(vit)\n",
    "\n",
    "        # ---- Load datasets & infer CSVs\n",
    "        test_ds = DementiaDetectionDataset(kind=\"test\", img_sz=IMG_SIZE)\n",
    "        fut_ds  = DementiaPredictionDataset(img_sz=IMG_SIZE)\n",
    "\n",
    "        test_csv = Path(infer) / \"test_preds.csv\"          # [idx, label, pred]\n",
    "        fut_csv  = Path(infer) / \"prediction_preds.csv\"    # [idx, pred, event, obs_time]\n",
    "        if not test_csv.exists() or not fut_csv.exists():\n",
    "            print(f\"[SKIP] Missing infer CSVs under {infer}\")\n",
    "            continue\n",
    "        test_csv = pd.read_csv(test_csv)\n",
    "        fut_csv  = pd.read_csv(fut_csv)\n",
    "\n",
    "        # ---- attach image paths\n",
    "        test_df = attach_imgpath(test_csv, test_ds)\n",
    "        fut_df  = attach_imgpath(fut_csv,  fut_ds)\n",
    "\n",
    "        # ---- merge eye_orientation\n",
    "        test_df = pd.merge(test_df, master_norm, on='img_path', how='inner')\n",
    "        fut_df  = pd.merge(fut_df,  master_norm, on='img_path', how='inner')\n",
    "\n",
    "        # ---- ensure attention capture (hooks → fallback override if needed)\n",
    "        try:\n",
    "            probe_path = (test_df[\"img_path\"].iloc[0] if len(test_df) else fut_df[\"img_path\"].iloc[0])\n",
    "            probe_tensor = load_tensor_01(ensure_path(probe_path)).unsqueeze(0).to(device)\n",
    "            ensure_attention_capture(model, vit, probe_tensor)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Attention probe failed: {e}\")\n",
    "\n",
    "        # ---- filter TP by BEST_THRESH\n",
    "        test_tp = test_df.query(\"pred >= @BEST_THRESH and label == 1\").reset_index(drop=True)\n",
    "        fut_tp  = fut_df .query(\"pred >= @BEST_THRESH and event == 1\").reset_index(drop=True)\n",
    "        print(f\"TP counts — TEST: {len(test_tp)} | FUTURE: {len(fut_tp)}\")\n",
    "\n",
    "        # ---- export origin_images.pth & saliency_map.pth\n",
    "        ok_test = export_tp_pack(test_tp, OUT_ROOT_TEST, rollout, model)\n",
    "        ok_fut  = export_tp_pack(fut_tp,  OUT_ROOT_FUTURE, rollout, model)\n",
    "\n",
    "        # ---- save the TP CSVs\n",
    "        if len(test_tp): test_tp.to_csv(Path(OUT_ROOT_TEST)/\"test_df.csv\", index=False)\n",
    "        if len(fut_tp):  fut_tp.to_csv(Path(OUT_ROOT_FUTURE)/\"fut_df.csv\", index=False)\n",
    "\n",
    "        # ---- representative Grad-CAM images (L/R heatmap & overlay)\n",
    "        if ok_test:   save_heatmap_and_overlay(OUT_ROOT_TEST,   desc, set_name=\"TEST\",   seed=123)\n",
    "        if ok_fut:    save_heatmap_and_overlay(OUT_ROOT_FUTURE, desc, set_name=\"FUTURE\", seed=456)\n",
    "\n",
    "        # ---- Mask metrics + Attention summaries\n",
    "        if ok_test:\n",
    "            test_df_csv   = Path(OUT_ROOT_TEST) / \"test_df.csv\"\n",
    "            test_saliency = Path(OUT_ROOT_TEST) / \"saliency_map.pth\"\n",
    "            test_out_dir  = Path(OUT_ROOT_TEST) / \"Mask\"\n",
    "            test_results  = compute_metrics_for_dataset(test_df_csv, test_saliency, test_out_dir)\n",
    "            test_ratios, test_summary = summarize_attention(test_results, label=f\"TEST:{desc}\", n_boot=2000)\n",
    "            test_summary.to_csv(test_out_dir / \"ATTN_summary_TEST.csv\")\n",
    "        if ok_fut:\n",
    "            fut_df_csv   = Path(OUT_ROOT_FUTURE) / \"fut_df.csv\"\n",
    "            fut_saliency = Path(OUT_ROOT_FUTURE) / \"saliency_map.pth\"\n",
    "            fut_out_dir  = Path(OUT_ROOT_FUTURE) / \"Mask\"\n",
    "            fut_results  = compute_metrics_for_dataset(fut_df_csv, fut_saliency, fut_out_dir)\n",
    "            fut_ratios,  fut_summary  = summarize_attention(fut_results,  label=f\"FUTURE:{desc}\", n_boot=2000)\n",
    "            fut_summary.to_csv(fut_out_dir / \"ATTN_summary_FUTURE.csv\")\n",
    "\n",
    "print(\"\\n✅ Done for all combos (hooks + fallback override w/ RoPE; non-square token grid safe; FOV-safe averaging)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51464e1-c872-429c-8555-6ef5c94dd6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automorph",
   "language": "python",
   "name": "automorph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
